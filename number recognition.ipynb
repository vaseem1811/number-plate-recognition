{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpRoetzRMv6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from Image_Generator import TextImageGenerator\n",
        "from Model import get_Model\n",
        "from parameter import *\n",
        "K.set_learning_phase(0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGarYuB1MzR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = get_Model(training=True)\n",
        "\n",
        "try:\n",
        "    model.load_weights('LSTM+BN4--26--0.011.hdf5')\n",
        "    print(\"...Previous weight data...\")\n",
        "except:\n",
        "    print(\"...New weight data...\")\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhx0STogM2xF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file_path = './DB/train/'\n",
        "tiger_train = TextImageGenerator(train_file_path, img_w, img_h, batch_size, downsample_factor)\n",
        "tiger_train.build_data()\n",
        "\n",
        "valid_file_path = './DB/test/'\n",
        "tiger_val = TextImageGenerator(valid_file_path, img_w, img_h, val_batch_size, downsample_factor)\n",
        "tiger_val.build_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz425ZAKM7L_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ada = Adadelta()\n",
        "\n",
        "early_stop = EarlyStopping(monitor='loss', min_delta=0.001, patience=4, mode='min', verbose=1)\n",
        "checkpoint = ModelCheckpoint(filepath='LSTM+BN5--{epoch:02d}--{val_loss:.3f}.hdf5', monitor='loss', verbose=1, mode='min', period=1)\n",
        "# the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n",
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=ada)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k60eW_rM-9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(generator=tiger_train.next_batch(),\n",
        "                    steps_per_epoch=int(tiger_train.n / batch_size),\n",
        "                    epochs=30,\n",
        "                    callbacks=[checkpoint],\n",
        "                    validation_data=tiger_val.next_batch(),\n",
        "                    validation_steps=int(tiger_val.n / val_batch_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utfvl2NSNU-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CHAR_VECTOR = \"adefghjknqrstwABCDEFGHIJKLMNOPZ0123456789\"\n",
        "\n",
        "letters = [letter for letter in CHAR_VECTOR]\n",
        "\n",
        "num_classes = len(letters) + 1\n",
        "\n",
        "img_w, img_h = 128, 64\n",
        "\n",
        "# Network parameters\n",
        "batch_size = 128\n",
        "val_batch_size = 16\n",
        "\n",
        "downsample_factor = 4\n",
        "max_text_len = 9"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}